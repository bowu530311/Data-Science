{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Bob\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - transformers\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    regex-2020.11.13           |   py37hcc03f2d_1         347 KB  conda-forge\n",
      "    sacremoses-0.0.43          |     pyh9f0ad1d_0         430 KB  conda-forge\n",
      "    tokenizers-0.9.4           |   py37h537c2b9_1         1.8 MB  conda-forge\n",
      "    transformers-4.3.2         |     pyhd8ed1ab_0         907 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  regex              conda-forge/win-64::regex-2020.11.13-py37hcc03f2d_1\n",
      "  sacremoses         conda-forge/noarch::sacremoses-0.0.43-pyh9f0ad1d_0\n",
      "  tokenizers         conda-forge/win-64::tokenizers-0.9.4-py37h537c2b9_1\n",
      "  transformers       conda-forge/noarch::transformers-4.3.2-pyhd8ed1ab_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "sacremoses-0.0.43    | 430 KB    |            |   0% \n",
      "sacremoses-0.0.43    | 430 KB    | 3          |   4% \n",
      "sacremoses-0.0.43    | 430 KB    | ####       |  41% \n",
      "sacremoses-0.0.43    | 430 KB    | #######8   |  78% \n",
      "sacremoses-0.0.43    | 430 KB    | ########## | 100% \n",
      "sacremoses-0.0.43    | 430 KB    | ########## | 100% \n",
      "\n",
      "transformers-4.3.2   | 907 KB    |            |   0% \n",
      "transformers-4.3.2   | 907 KB    | 1          |   2% \n",
      "transformers-4.3.2   | 907 KB    | #9         |  19% \n",
      "transformers-4.3.2   | 907 KB    | ##8        |  28% \n",
      "transformers-4.3.2   | 907 KB    | #####8     |  58% \n",
      "transformers-4.3.2   | 907 KB    | #######5   |  76% \n",
      "transformers-4.3.2   | 907 KB    | #########1 |  92% \n",
      "transformers-4.3.2   | 907 KB    | ########## | 100% \n",
      "\n",
      "regex-2020.11.13     | 347 KB    |            |   0% \n",
      "regex-2020.11.13     | 347 KB    | ##3        |  23% \n",
      "regex-2020.11.13     | 347 KB    | ######9    |  69% \n",
      "regex-2020.11.13     | 347 KB    | ########## | 100% \n",
      "regex-2020.11.13     | 347 KB    | ########## | 100% \n",
      "\n",
      "tokenizers-0.9.4     | 1.8 MB    |            |   0% \n",
      "tokenizers-0.9.4     | 1.8 MB    |            |   1% \n",
      "tokenizers-0.9.4     | 1.8 MB    | 9          |  10% \n",
      "tokenizers-0.9.4     | 1.8 MB    | #5         |  15% \n",
      "tokenizers-0.9.4     | 1.8 MB    | ##3        |  24% \n",
      "tokenizers-0.9.4     | 1.8 MB    | ###2       |  33% \n",
      "tokenizers-0.9.4     | 1.8 MB    | ####1      |  42% \n",
      "tokenizers-0.9.4     | 1.8 MB    | #####      |  50% \n",
      "tokenizers-0.9.4     | 1.8 MB    | #####9     |  59% \n",
      "tokenizers-0.9.4     | 1.8 MB    | ######8    |  68% \n",
      "tokenizers-0.9.4     | 1.8 MB    | #######7   |  78% \n",
      "tokenizers-0.9.4     | 1.8 MB    | ########6  |  87% \n",
      "tokenizers-0.9.4     | 1.8 MB    | #########5 |  96% \n",
      "tokenizers-0.9.4     | 1.8 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('Kaggle Movie Sentiment/train.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['Phrase']\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cc,(x,y,z) in enumerate((zip(a['SentenceId'],a['Phrase'],a['Sentiment']))):\n",
    "#     print(\"cc: {}\".format(cc))\n",
    "#     print(\"x: {}\".format(x))\n",
    "#     print(y)\n",
    "#     print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "label=[]\n",
    "num=[]\n",
    "for h,(x,y,z) in enumerate((zip(a['SentenceId'],a['Phrase'],a['Sentiment']))):\n",
    "#     print(a)\n",
    "#     print(x)\n",
    "#     print(y)\n",
    "#     print(z)\n",
    "#     break\n",
    "\n",
    "    num.append(x)\n",
    "    if x==0:\n",
    "        corpus.append(y)\n",
    "        label.append(z)\n",
    "    elif x!=num[h-1]:\n",
    "        corpus.append(y)\n",
    "        label.append(z)\n",
    "        \n",
    "#this is because there are a large number of abanden data so we dont want to include those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8528\n",
      "8528\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopword = stopwords.words('english')\n",
    "spacy_stopword = STOP_WORDS\n",
    "total_stopword = set(nltk_stopword).union(spacy_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExcepStopWords = {\n",
    "    'again',\n",
    "    'against',\n",
    "    'ain',\n",
    "    'almost',\n",
    "    'among',\n",
    "    'amongst',\n",
    "    'amount',\n",
    "    'anyhow',\n",
    "    'anyway',\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'below',\n",
    "    'bottom',\n",
    "    'but',\n",
    "    'cannot',\n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'don',\n",
    "    \"don't\",\n",
    "    'done',\n",
    "    'down',\n",
    "    'except',\n",
    "    'few',\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'hasn',\n",
    "    \"hasn't\",\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'however',\n",
    "    'isn',\n",
    "    \"isn't\",\n",
    "    'least',\n",
    "    'mightn',\n",
    "    \"mightn't\",\n",
    "    'must',\n",
    "    'mustn',\n",
    "    \"mustn't\",\n",
    "    'needn',\n",
    "    \"needn't\",\n",
    "    'neither',\n",
    "    'never',\n",
    "    'nevertheless',\n",
    "    'no',\n",
    "    'nobody',\n",
    "    'none',\n",
    "    'noone',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'nothing',\n",
    "    'should',\n",
    "    \"should've\",\n",
    "    'shouldn',\n",
    "    \"shouldn't\",\n",
    "    'too',\n",
    "    'top',\n",
    "    'up',\n",
    "    'wasn',\n",
    "    \"wasn't\",\n",
    "    'well',\n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    'won',\n",
    "    \"won't\",\n",
    "    'wouldn',\n",
    "    \"wouldn't\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mystopword = total_stopword - ExcepStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mytokenizer(data):\n",
    "    aa = tokenizer.tokenize(data)\n",
    "    return [x.lower() for x in aa]\n",
    "\n",
    "def remove_stop(data):\n",
    "    return [x for x in data if x not in Mystopword]\n",
    "\n",
    "nlp = spacy.load('en',disable=['parser','tagger','ner'])\n",
    "def my_lemma(data):\n",
    "    mylist =[]\n",
    "    for x in data:\n",
    "        y = nlp(x)\n",
    "        for z in y:\n",
    "            mylist.append(z.lemma_)\n",
    "    return mylist\n",
    "\n",
    "def pipline(data):\n",
    "    review = mytokenizer(data)\n",
    "    review = remove_stop(review)\n",
    "    review = my_lemma(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = list(map(pipline,corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even fans of Ismail Merchant 's work , I suspect , would have a hard time sitting through this one .\n",
      "['fan', 'ismail', 'merchant', \"'\", 'work', ',', 'suspect', ',', 'hard', 'time', 'sit', '.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[1])\n",
    "print(review[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8528"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_review=[]\n",
    "for x in review:\n",
    "    my_review.append(\" \".join(x))\n",
    "len(my_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,valid_x,train_y,valid_y = train_test_split(corpus,label,random_state=2018,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6822\n",
      "6822\n",
      "1706\n",
      "1706\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(valid_x))\n",
    "print(len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = pd.read_csv('Kaggle Movie Sentiment/test.tsv',sep='\\t') \n",
    "#since our test data has no labels, then we dont use it for our example here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_corp=[]\n",
    "# test_label=[]\n",
    "# for x, y in zip(b['Phrase'])\n",
    "#since our test data has no labels, then we dont use it for our example here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b3a47072bd40eea3c6b164bc34217b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417b6ca46b614a8ca381eb953c80605a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191d47044a684bac92dc7572fac52d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945c6978e1734928a814436126318e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load pretrained bert model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load bert tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample trying\n",
    "aaa=[]\n",
    "aaa.append(train_x[1])\n",
    "aaa.append(train_x[2])\n",
    "sample_id = tokenizer.batch_encode_plus(aaa,padding=True,return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The biggest problem with Satin Rouge is Lilia herself .'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1037, 11937, 21756, 16137, 7911, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2182, 2009, 2003, 1024, 2009, 1005, 1055, 2055, 1037, 2155, 1997, 14768, 26796, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25092951708>"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "seq_len = [len(x.split()) for x in train_x]\n",
    "pd.Series(seq_len).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6822\n"
     ]
    }
   ],
   "source": [
    "max_len=25\n",
    "train_x\n",
    "print(len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tokenizer convert a sentense into indexes and also add in attention masks\n",
    "\n",
    "\n",
    "train_token = tokenizer.batch_encode_plus(\n",
    "    train_x,\n",
    "    max_length = max_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6822\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_token = tokenizer.batch_encode_plus(\n",
    "    valid_x,\n",
    "    max_length = max_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we need to conver the tokenised sentense into tensor format\n",
    "\n",
    "train_x = torch.tensor(train_token['input_ids'])\n",
    "train_mask = torch.tensor(train_token['attention_mask'])\n",
    "train_y=torch.tensor(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.tensor(valid_token['input_ids'])\n",
    "valid_mask = torch.tensor(valid_token['attention_mask'])\n",
    "valid_y=torch.tensor(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "train_dataset = TensorDataset(train_x,train_mask,train_y)\n",
    "#train_sampler = RandomSampler(train_dataset)\n",
    "#train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=batch_size)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size)\n",
    "\n",
    "valid_dataset = TensorDataset(valid_x,valid_mask,valid_y)\n",
    "#valid_sampler = SequentialSampler(valid_dataset)\n",
    "#valid_dataloader = DataLoader(valid_dataset,sampler=valid_sampler,batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_dataset,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6822\n",
      "6822\n",
      "6822\n",
      "1706\n",
      "1706\n",
      "1706\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x))\n",
    "print(len(train_mask))\n",
    "print(len(train_y))\n",
    "      \n",
    "print(len(valid_x))\n",
    "print(len(valid_mask))\n",
    "print(len(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################BERT model#######################\n",
    "for para in bert.parameters():\n",
    "    para.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_bert(nn.Module):\n",
    "    def __init__(self,bert_net):\n",
    "        super(my_bert,self).__init__()\n",
    "        self.bert_net = bert_net\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512,5)\n",
    "        \n",
    "    def forward(self,data,mask):\n",
    "        \n",
    "        __,myoutput = self.bert_net(data,attention_mask=mask,return_dict=False)\n",
    "        output = self.fc1(myoutput)\n",
    "        #output = self.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = my_bert(bert)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = nn.CrossEntropyLoss()\n",
    "opt = AdamW(model.parameters(),lr=0.000005)\n",
    "#opti_scheduler=torch.optim.lr_scheduler.StepLR(opt,step_size=500,gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 20 with loss 1.24656\n",
      "Batch # 40 with loss 1.33624\n",
      "Batch # 60 with loss 1.19038\n",
      "Batch # 80 with loss 1.23058\n",
      "Batch # 100 with loss 1.36372\n",
      "Epoch #1 with accuracy of 45.76%\n",
      "Batch # 20 with loss 1.38060\n",
      "Batch # 40 with loss 1.35848\n",
      "Batch # 60 with loss 1.14412\n",
      "Batch # 80 with loss 1.26572\n",
      "Batch # 100 with loss 1.32265\n",
      "Epoch #2 with accuracy of 45.88%\n",
      "Batch # 20 with loss 1.34862\n",
      "Batch # 40 with loss 1.36963\n",
      "Batch # 60 with loss 1.15781\n",
      "Batch # 80 with loss 1.24918\n",
      "Batch # 100 with loss 1.37547\n",
      "Epoch #3 with accuracy of 45.22%\n",
      "Batch # 20 with loss 1.26042\n",
      "Batch # 40 with loss 1.36654\n",
      "Batch # 60 with loss 1.19020\n",
      "Batch # 80 with loss 1.21628\n",
      "Batch # 100 with loss 1.22150\n",
      "Epoch #4 with accuracy of 45.59%\n",
      "Batch # 20 with loss 1.27764\n",
      "Batch # 40 with loss 1.36710\n",
      "Batch # 60 with loss 1.14280\n",
      "Batch # 80 with loss 1.24147\n",
      "Batch # 100 with loss 1.32452\n",
      "Epoch #5 with accuracy of 44.83%\n",
      "Batch # 20 with loss 1.26442\n",
      "Batch # 40 with loss 1.31113\n",
      "Batch # 60 with loss 1.10466\n",
      "Batch # 80 with loss 1.24017\n",
      "Batch # 100 with loss 1.27358\n",
      "Epoch #6 with accuracy of 45.57%\n",
      "Batch # 20 with loss 1.33213\n",
      "Batch # 40 with loss 1.37569\n",
      "Batch # 60 with loss 1.13255\n",
      "Batch # 80 with loss 1.17586\n",
      "Batch # 100 with loss 1.31043\n",
      "Epoch #7 with accuracy of 45.97%\n",
      "Batch # 20 with loss 1.28079\n",
      "Batch # 40 with loss 1.40832\n",
      "Batch # 60 with loss 1.15174\n",
      "Batch # 80 with loss 1.27984\n",
      "Batch # 100 with loss 1.32965\n",
      "Epoch #8 with accuracy of 45.25%\n",
      "Batch # 20 with loss 1.18833\n",
      "Batch # 40 with loss 1.37852\n",
      "Batch # 60 with loss 1.09373\n",
      "Batch # 80 with loss 1.25623\n",
      "Batch # 100 with loss 1.39910\n",
      "Epoch #9 with accuracy of 45.66%\n",
      "Batch # 20 with loss 1.23929\n",
      "Batch # 40 with loss 1.38400\n",
      "Batch # 60 with loss 1.11950\n",
      "Batch # 80 with loss 1.25895\n",
      "Batch # 100 with loss 1.39377\n",
      "Epoch #10 with accuracy of 44.66%\n",
      "Batch # 20 with loss 1.27257\n",
      "Batch # 40 with loss 1.42036\n",
      "Batch # 60 with loss 1.17823\n",
      "Batch # 80 with loss 1.24076\n",
      "Batch # 100 with loss 1.25830\n",
      "Epoch #11 with accuracy of 45.62%\n",
      "Batch # 20 with loss 1.29656\n",
      "Batch # 40 with loss 1.30188\n",
      "Batch # 60 with loss 1.14557\n",
      "Batch # 80 with loss 1.25523\n",
      "Batch # 100 with loss 1.35072\n",
      "Epoch #12 with accuracy of 45.15%\n",
      "Batch # 20 with loss 1.27967\n",
      "Batch # 40 with loss 1.33478\n",
      "Batch # 60 with loss 1.11083\n",
      "Batch # 80 with loss 1.30530\n",
      "Batch # 100 with loss 1.36955\n",
      "Epoch #13 with accuracy of 44.69%\n",
      "Batch # 20 with loss 1.31484\n",
      "Batch # 40 with loss 1.39389\n",
      "Batch # 60 with loss 1.18601\n",
      "Batch # 80 with loss 1.31785\n",
      "Batch # 100 with loss 1.31910\n",
      "Epoch #14 with accuracy of 45.72%\n",
      "Batch # 20 with loss 1.33556\n",
      "Batch # 40 with loss 1.35671\n",
      "Batch # 60 with loss 1.15396\n",
      "Batch # 80 with loss 1.26876\n",
      "Batch # 100 with loss 1.36172\n",
      "Epoch #15 with accuracy of 45.75%\n",
      "Batch # 20 with loss 1.31430\n",
      "Batch # 40 with loss 1.38565\n",
      "Batch # 60 with loss 1.17605\n",
      "Batch # 80 with loss 1.13172\n",
      "Batch # 100 with loss 1.28680\n",
      "Epoch #16 with accuracy of 44.75%\n",
      "Batch # 20 with loss 1.24170\n",
      "Batch # 40 with loss 1.38639\n",
      "Batch # 60 with loss 1.12632\n",
      "Batch # 80 with loss 1.24457\n",
      "Batch # 100 with loss 1.33554\n",
      "Epoch #17 with accuracy of 45.50%\n",
      "Batch # 20 with loss 1.30088\n",
      "Batch # 40 with loss 1.37305\n",
      "Batch # 60 with loss 1.09751\n",
      "Batch # 80 with loss 1.18779\n",
      "Batch # 100 with loss 1.33943\n",
      "Epoch #18 with accuracy of 45.22%\n",
      "Batch # 20 with loss 1.27969\n",
      "Batch # 40 with loss 1.37673\n",
      "Batch # 60 with loss 1.10207\n",
      "Batch # 80 with loss 1.19022\n",
      "Batch # 100 with loss 1.34730\n",
      "Epoch #19 with accuracy of 46.20%\n",
      "Batch # 20 with loss 1.29362\n",
      "Batch # 40 with loss 1.41201\n",
      "Batch # 60 with loss 1.15727\n",
      "Batch # 80 with loss 1.19519\n",
      "Batch # 100 with loss 1.27400\n",
      "Epoch #20 with accuracy of 45.02%\n",
      "Batch # 20 with loss 1.25442\n",
      "Batch # 40 with loss 1.40590\n",
      "Batch # 60 with loss 1.11519\n",
      "Batch # 80 with loss 1.22826\n",
      "Batch # 100 with loss 1.30576\n",
      "Epoch #21 with accuracy of 44.37%\n",
      "Batch # 20 with loss 1.25913\n",
      "Batch # 40 with loss 1.38711\n",
      "Batch # 60 with loss 1.15416\n",
      "Batch # 80 with loss 1.24680\n",
      "Batch # 100 with loss 1.33002\n",
      "Epoch #22 with accuracy of 45.38%\n",
      "Batch # 20 with loss 1.29408\n",
      "Batch # 40 with loss 1.36562\n",
      "Batch # 60 with loss 1.10699\n",
      "Batch # 80 with loss 1.20969\n",
      "Batch # 100 with loss 1.33558\n",
      "Epoch #23 with accuracy of 45.53%\n",
      "Batch # 20 with loss 1.34924\n",
      "Batch # 40 with loss 1.40652\n",
      "Batch # 60 with loss 1.11972\n",
      "Batch # 80 with loss 1.15862\n",
      "Batch # 100 with loss 1.30905\n",
      "Epoch #24 with accuracy of 45.56%\n",
      "Batch # 20 with loss 1.23157\n",
      "Batch # 40 with loss 1.33224\n",
      "Batch # 60 with loss 1.11632\n",
      "Batch # 80 with loss 1.19084\n",
      "Batch # 100 with loss 1.34302\n",
      "Epoch #25 with accuracy of 45.69%\n",
      "Batch # 20 with loss 1.29829\n",
      "Batch # 40 with loss 1.37061\n",
      "Batch # 60 with loss 1.14468\n",
      "Batch # 80 with loss 1.30772\n",
      "Batch # 100 with loss 1.29520\n",
      "Epoch #26 with accuracy of 45.32%\n",
      "Batch # 20 with loss 1.41526\n",
      "Batch # 40 with loss 1.31003\n",
      "Batch # 60 with loss 1.09930\n",
      "Batch # 80 with loss 1.26106\n",
      "Batch # 100 with loss 1.28465\n",
      "Epoch #27 with accuracy of 45.79%\n",
      "Batch # 20 with loss 1.18123\n",
      "Batch # 40 with loss 1.42753\n",
      "Batch # 60 with loss 1.14266\n",
      "Batch # 80 with loss 1.23318\n",
      "Batch # 100 with loss 1.27088\n",
      "Epoch #28 with accuracy of 45.59%\n",
      "Batch # 20 with loss 1.30310\n",
      "Batch # 40 with loss 1.35658\n",
      "Batch # 60 with loss 1.09829\n",
      "Batch # 80 with loss 1.19330\n",
      "Batch # 100 with loss 1.34472\n",
      "Epoch #29 with accuracy of 45.38%\n",
      "Batch # 20 with loss 1.27570\n",
      "Batch # 40 with loss 1.27746\n",
      "Batch # 60 with loss 1.16283\n",
      "Batch # 80 with loss 1.26601\n",
      "Batch # 100 with loss 1.37798\n",
      "Epoch #30 with accuracy of 45.06%\n",
      "Batch # 20 with loss 1.30070\n",
      "Batch # 40 with loss 1.36294\n",
      "Batch # 60 with loss 1.16067\n",
      "Batch # 80 with loss 1.18354\n",
      "Batch # 100 with loss 1.26489\n",
      "Epoch #31 with accuracy of 45.76%\n",
      "Batch # 20 with loss 1.28656\n",
      "Batch # 40 with loss 1.36128\n",
      "Batch # 60 with loss 1.22736\n",
      "Batch # 80 with loss 1.20070\n",
      "Batch # 100 with loss 1.37078\n",
      "Epoch #32 with accuracy of 45.79%\n",
      "Batch # 20 with loss 1.27026\n",
      "Batch # 40 with loss 1.33661\n",
      "Batch # 60 with loss 1.14928\n",
      "Batch # 80 with loss 1.27805\n",
      "Batch # 100 with loss 1.30545\n",
      "Epoch #33 with accuracy of 45.49%\n",
      "Batch # 20 with loss 1.28232\n",
      "Batch # 40 with loss 1.37928\n",
      "Batch # 60 with loss 1.09274\n",
      "Batch # 80 with loss 1.22985\n",
      "Batch # 100 with loss 1.34531\n",
      "Epoch #34 with accuracy of 45.34%\n",
      "Batch # 20 with loss 1.31259\n",
      "Batch # 40 with loss 1.38848\n",
      "Batch # 60 with loss 1.11334\n",
      "Batch # 80 with loss 1.15260\n",
      "Batch # 100 with loss 1.34834\n",
      "Epoch #35 with accuracy of 45.95%\n",
      "Batch # 20 with loss 1.30625\n",
      "Batch # 40 with loss 1.38661\n",
      "Batch # 60 with loss 1.12775\n",
      "Batch # 80 with loss 1.20219\n",
      "Batch # 100 with loss 1.30032\n",
      "Epoch #36 with accuracy of 45.68%\n",
      "Batch # 20 with loss 1.31532\n",
      "Batch # 40 with loss 1.33261\n",
      "Batch # 60 with loss 1.12761\n",
      "Batch # 80 with loss 1.22783\n",
      "Batch # 100 with loss 1.40688\n",
      "Epoch #37 with accuracy of 45.91%\n",
      "Batch # 20 with loss 1.27478\n",
      "Batch # 40 with loss 1.37279\n",
      "Batch # 60 with loss 1.14046\n",
      "Batch # 80 with loss 1.12072\n",
      "Batch # 100 with loss 1.33768\n",
      "Epoch #38 with accuracy of 45.76%\n",
      "Batch # 20 with loss 1.27924\n",
      "Batch # 40 with loss 1.32974\n",
      "Batch # 60 with loss 1.10982\n",
      "Batch # 80 with loss 1.22053\n",
      "Batch # 100 with loss 1.34778\n",
      "Epoch #39 with accuracy of 45.72%\n",
      "Batch # 20 with loss 1.29756\n",
      "Batch # 40 with loss 1.39639\n",
      "Batch # 60 with loss 1.12133\n",
      "Batch # 80 with loss 1.21921\n",
      "Batch # 100 with loss 1.34555\n",
      "Epoch #40 with accuracy of 44.56%\n",
      "Batch # 20 with loss 1.26268\n",
      "Batch # 40 with loss 1.37912\n",
      "Batch # 60 with loss 1.12474\n",
      "Batch # 80 with loss 1.24114\n",
      "Batch # 100 with loss 1.36433\n",
      "Epoch #41 with accuracy of 45.54%\n",
      "Batch # 20 with loss 1.31361\n",
      "Batch # 40 with loss 1.34072\n",
      "Batch # 60 with loss 1.16313\n",
      "Batch # 80 with loss 1.24407\n",
      "Batch # 100 with loss 1.30708\n",
      "Epoch #42 with accuracy of 45.60%\n",
      "Batch # 20 with loss 1.35932\n",
      "Batch # 40 with loss 1.37168\n",
      "Batch # 60 with loss 1.09921\n",
      "Batch # 80 with loss 1.22450\n",
      "Batch # 100 with loss 1.30247\n",
      "Epoch #43 with accuracy of 45.47%\n",
      "Batch # 20 with loss 1.31804\n",
      "Batch # 40 with loss 1.34402\n",
      "Batch # 60 with loss 1.09827\n",
      "Batch # 80 with loss 1.19968\n",
      "Batch # 100 with loss 1.33252\n",
      "Epoch #44 with accuracy of 45.63%\n",
      "Batch # 20 with loss 1.30565\n",
      "Batch # 40 with loss 1.38334\n",
      "Batch # 60 with loss 1.21698\n",
      "Batch # 80 with loss 1.27145\n",
      "Batch # 100 with loss 1.29335\n",
      "Epoch #45 with accuracy of 45.71%\n",
      "Batch # 20 with loss 1.29455\n",
      "Batch # 40 with loss 1.33546\n",
      "Batch # 60 with loss 1.12782\n",
      "Batch # 80 with loss 1.25610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch # 100 with loss 1.26059\n",
      "Epoch #46 with accuracy of 44.77%\n",
      "Batch # 20 with loss 1.26478\n",
      "Batch # 40 with loss 1.36858\n",
      "Batch # 60 with loss 1.19956\n",
      "Batch # 80 with loss 1.21861\n",
      "Batch # 100 with loss 1.31538\n",
      "Epoch #47 with accuracy of 46.01%\n",
      "Batch # 20 with loss 1.38696\n",
      "Batch # 40 with loss 1.38253\n",
      "Batch # 60 with loss 1.13676\n",
      "Batch # 80 with loss 1.24767\n",
      "Batch # 100 with loss 1.32375\n",
      "Epoch #48 with accuracy of 45.60%\n",
      "Batch # 20 with loss 1.22745\n",
      "Batch # 40 with loss 1.31478\n",
      "Batch # 60 with loss 1.16346\n",
      "Batch # 80 with loss 1.23676\n",
      "Batch # 100 with loss 1.32411\n",
      "Epoch #49 with accuracy of 45.25%\n",
      "Batch # 20 with loss 1.30731\n",
      "Batch # 40 with loss 1.33342\n",
      "Batch # 60 with loss 1.18273\n",
      "Batch # 80 with loss 1.25247\n",
      "Batch # 100 with loss 1.31489\n",
      "Epoch #50 with accuracy of 45.76%\n"
     ]
    }
   ],
   "source": [
    "epoch=50\n",
    "for a in range(epoch):\n",
    "    model.train()\n",
    "    acc=0\n",
    "    for x, bundled in enumerate(train_dataloader):\n",
    "        \n",
    "        newdata = [n.to(device) for n in bundled]\n",
    "        data,mask,y = newdata\n",
    "        pred = model(data,mask)\n",
    "        loss = losses(pred,y)\n",
    "        #opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        #opti_scheduler.step()\n",
    "        opt.zero_grad()\n",
    "        prediction = torch.argmax(pred,dim=1) #this return the indices of the max value\n",
    "        #or we can use: __,prediction = torch.max(pred,dim=1)\n",
    "        accu = torch.sum(prediction==y).item()\n",
    "        acc += accu\n",
    "        if (x+1)%20==0:\n",
    "            print('Batch # {} with loss {:.5f}'.format(x+1,loss))\n",
    "            #print('lr rate: {}'.format(opti_scheduler.get_last_lr()))\n",
    "    \n",
    "    print('Epoch #{} with accuracy of {:.2%}'.format(a+1,acc/(len(train_x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset accuracy 47.713951%\n"
     ]
    }
   ],
   "source": [
    "acc=0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for aa in valid_dataloader:\n",
    "        aaa = [x.to(device) for x in aa]\n",
    "        x,mask,y = aaa\n",
    "        pred = model(x,mask)\n",
    "        prediction = torch.argmax(pred,dim=1)\n",
    "        accu = torch.sum(prediction==y).item()\n",
    "        acc += accu\n",
    "    print('Validation dataset accuracy {:2%}'.format(acc/len(valid_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
